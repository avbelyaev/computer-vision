# -*- coding: utf-8 -*-
"""face_classifier_neural_network.ipynb

Automatically generated by Colaboratory.

ALL HAIL DATA SCIENCE
IMPORT * FROM *
"""

import zipfile
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import random_split
from torchvision.transforms import transforms
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
from torchvision.utils import make_grid

with zipfile.ZipFile('data 2.zip', 'r') as zip_ref:
    zip_ref.extractall('.')


def show_image(path):
    img = Image.open(path)
    img_arr = np.array(img)
    plt.figure(figsize=(5, 5))
    plt.imshow(np.transpose(img_arr, (0, 1, 2)))


transformations = transforms.Compose([
    transforms.RandomResizedCrop(64),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

total_dataset = datasets.ImageFolder('data', transform=transformations)
dataset_loader = DataLoader(dataset=total_dataset, batch_size=100)
# --> {'Bill_Simon': 0, 'Jennifer_Aniston': 1, 'LeBron_James': 2}



def show_transformed_image(image):
    np_image = image.numpy()
    plt.figure(figsize=(20, 20))
    plt.imshow(np.transpose(np_image, (1, 2, 0)))


show_transformed_image(make_grid(image))

total_dataset.class_to_idx



train_size = int(0.8 * len(total_dataset))
test_size = len(total_dataset) - train_size
train_dataset, test_dataset = random_split(total_dataset, [train_size, test_size])

train_dataset_loader = DataLoader(dataset=train_dataset, batch_size=10)
test_dataset_loader = DataLoader(dataset=test_dataset, batch_size=10)




class FaceClassifierX(nn.Module):

    def __init__(self, num_classes=5):
        super(FaceClassifierX, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.lf = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)

    def forward(self, input):
        output = self.conv1(input)
        output = self.relu1(output)
        output = self.maxpool1(output)
        output = self.conv2(output)
        output = self.relu2(output)
        output = output.view(-1, 32 * 32 * 24)
        output = self.lf(output)
        return output


model = FaceClassifierX(num_classes=3)
optimizer = Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss()


def train_and_build(n_epoches):
    for epoch in range(n_epoches):
        model.train()
        for i, (images, labels) in enumerate(train_dataset_loader):
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()


# запускаем на 30 эпох
train_and_build(30)

model.eval()
test_acc_count = 0
for k, (test_images, test_labels) in enumerate(test_dataset_loader):
    test_outputs = model(test_images)
    _, prediction = torch.max(test_outputs.data, 1)
    test_acc_count += torch.sum(prediction == test_labels.data).item()

test_accuracy = test_acc_count / len(test_dataset)

test_accuracy
# 0.8888888888888888

test_image = Image.open("/content/data/Jennifer_Aniston/Jennifer_Aniston_0001.jpg")
test_image_tensor = transformations(test_image).float()
test_image_tensor = test_image_tensor.unsqueeze_(0)
output = model(test_image_tensor)

classified_as_idx = output.data.numpy().argmax()
print(classified_as_idx)

for name, idx in total_dataset.class_to_idx.items():
    if idx == classified_as_idx:
        print(name)
        break
# 1
# Jennifer_Aniston
